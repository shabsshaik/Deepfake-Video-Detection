{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc7204a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd29c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import moviepy.editor as mp\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7bfd490",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_zip = r\"C:\\Users\\shaba\\Downloads\\FINAL-EFFICIENTNETV2-B0.zip\"\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall( r\"C:\\Users\\shaba\\Downloads\\FINAL-EFFICIENTNETV2-B0\")\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42e28257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face detector\n",
    "mtcnn = MTCNN(margin=14, keep_all=True, factor=0.7, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6b42c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionPipeline:\n",
    "    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\n",
    "\n",
    "    def __init__(self, detector, n_frames=40, batch_size=60, resize=None):\n",
    "        \"\"\"Constructor for DetectionPipeline class.\n",
    "        Keyword Arguments:\n",
    "            n_frames {int} -- Total number of frames to load. These will be evenly spaced\n",
    "                throughout the video. If not specified (i.e., None), all frames will be loaded.\n",
    "                (default: {None})\n",
    "            batch_size {int} -- Batch size to use with MTCNN face detector. (default: {32})\n",
    "            resize {float} -- Fraction by which to resize frames from original prior to face\n",
    "                detection. A value less than 1 results in downsampling and a value greater than\n",
    "                1 result in upsampling. (default: {None})\n",
    "        \"\"\"\n",
    "        self.detector = detector\n",
    "        self.n_frames = n_frames\n",
    "        self.batch_size = batch_size\n",
    "        self.resize = resize\n",
    "\n",
    "    def __call__(self, filename):\n",
    "        \"\"\"Load frames from an MP4 video and detect faces.\n",
    "        Arguments:\n",
    "            filename {str} -- Path to video.\n",
    "        \"\"\"\n",
    "        # Create video reader and find length\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Pick 'n_frames' evenly spaced frames to sample\n",
    "        if self.n_frames is None:\n",
    "            sample = np.arange(0, v_len)\n",
    "        else:\n",
    "            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n",
    "\n",
    "        # Loop through frames\n",
    "        faces = []\n",
    "        frames = []\n",
    "        for j in range(v_len):\n",
    "            success = v_cap.grab()\n",
    "            if j in sample:\n",
    "                # Load frame\n",
    "                success, frame = v_cap.retrieve()\n",
    "                if not success:\n",
    "                    continue\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                # frame = Image.fromarray(frame)\n",
    "\n",
    "                # Resize frame to desired size\n",
    "                if self.resize is not None:\n",
    "                    frame = frame.resize([int(d * self.resize) for d in frame.size])\n",
    "                frames.append(frame)\n",
    "\n",
    "                # When batch is full, detect faces and reset frame list\n",
    "                if len(frames) % self.batch_size == 0 or j == sample[-1]:\n",
    "\n",
    "                    boxes, probs = self.detector.detect(frames)\n",
    "\n",
    "                    for i in range(len(frames)):\n",
    "\n",
    "                        if boxes[i] is None:\n",
    "                            faces.append(face2)     #append previous face frame if no face is detected\n",
    "                            continue\n",
    "\n",
    "                        box = boxes[i][0].astype(int)\n",
    "                        frame = frames[i]\n",
    "                        face = frame[box[1]:box[3], box[0]:box[2]]\n",
    "\n",
    "                        if not face.any():\n",
    "                            faces.append(face2)     #append previous face frame if no face is detected\n",
    "                            continue\n",
    "\n",
    "                        face2 = cv2.resize(face, (224, 224))\n",
    "\n",
    "                        faces.append(face2)\n",
    "\n",
    "                    frames = []\n",
    "\n",
    "        v_cap.release()\n",
    "\n",
    "        return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7350535",
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_pipeline = DetectionPipeline(detector=mtcnn,n_frames=20, batch_size=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d64c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(r\"C:\\Users\\shaba\\Downloads\\FINAL-EFFICIENTNETV2-B0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "995ec981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepfakespredict(input_video):\n",
    "\n",
    "    faces = detection_pipeline(input_video)\n",
    "\n",
    "    total = 0\n",
    "    real = 0\n",
    "    fake = 0\n",
    "\n",
    "    for face in faces:\n",
    "\n",
    "        face2 = face/255\n",
    "        pred = model.predict(np.expand_dims(face2, axis=0))[0]\n",
    "        total+=1\n",
    "\n",
    "        pred2 = pred[1]\n",
    "\n",
    "        if pred2 > 0.5:\n",
    "          fake+=1\n",
    "        else:\n",
    "          real+=1\n",
    "\n",
    "    fake_ratio = fake/total\n",
    "\n",
    "    text =\"\"\n",
    "    text2 = \"Deepfakes Confidence: \" + str(fake_ratio*100) + \"%\"\n",
    "\n",
    "    if fake_ratio >= 0.5:\n",
    "        text = \"The video is FAKE.\"\n",
    "    else:\n",
    "        text = \"The video is REAL.\"\n",
    "\n",
    "    face_frames = []\n",
    "    \n",
    "    for face in faces:\n",
    "        face_frame = Image.fromarray(face.astype('uint8'), 'RGB')\n",
    "        face_frames.append(face_frame)\n",
    "        \n",
    "    face_frames[0].save('results.gif', save_all=True, append_images=face_frames[1:], duration = 250, loop = 100 )\n",
    "    clip = mp.VideoFileClip(\"results.gif\")\n",
    "    clip.write_videofile(\"video.mp4\")\n",
    "\n",
    "    return text, text2, \"video.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1e814be",
   "metadata": {},
   "outputs": [],
   "source": [
    "title=\"SURGE Deepfakes Video Detector\"\n",
    "description=\"This is a Video Deepfakes Detector by using frame-by-frame detection.  To use it, simply upload your video, or click one of the examples to load them.\"\n",
    "            \n",
    "# examples = [              \n",
    "#                 ['Video1-fake-1-ff.mp4'],\n",
    "#                 ['Video6-real-1-ff.mp4'],\n",
    "#                 ['Video3-fake-3-ff.mp4'],\n",
    "#                 ['Video8-real-3-ff.mp4'],\n",
    "#                 ['real-1.mp4'],\n",
    "#                 ['fake-1.mp4'],\n",
    "#            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0d2fa447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thanks for being a Gradio user! If you have questions or feedback, please join our Discord server and chat with us: https://discord.gg/feTf9x3ZSB\n",
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 192ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 171ms/step\n",
      "1/1 [==============================] - 0s 175ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 176ms/step\n",
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 145ms/step\n",
      "1/1 [==============================] - 0s 155ms/step\n",
      "1/1 [==============================] - 0s 162ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 163ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "1/1 [==============================] - 0s 168ms/step\n",
      "1/1 [==============================] - 0s 164ms/step\n",
      "Moviepy - Building video video.mp4.\n",
      "Moviepy - Writing video video.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready video.mp4\n"
     ]
    }
   ],
   "source": [
    "gr.Interface(deepfakespredict,\n",
    "                    inputs=\"video\",\n",
    "                     outputs=[\"text\",\"text\", gr.outputs.Video(label=\"Detected face sequence\")],\n",
    "                     title=title,\n",
    "                     description=description,\n",
    "#                      examples=examples\n",
    "                     ).launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c0a949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8383d244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
